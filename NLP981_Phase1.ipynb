{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP981_Phase1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tduc6QDQz1H"
      },
      "source": [
        "# NLP981 Final Project - Phase #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F0hIUs7oCS"
      },
      "source": [
        "*   Instructor: Javad PourMostafa\n",
        "*   Teaching Assistant: Parsa Abbasi\n",
        "*   University of Guilan, 1st semester of 2019\n",
        "*   GitHub repository : *https://github.com/JoyeBright/NLP*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVC1mwiaOZMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e0f1320-1126-4600-a34e-6479d6720e31"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install hazm\n",
        "!pip install stopwords_guilannlp\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 5.6MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 17.7MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394468 sha256=76faa268a0f1ccec86a9f92492ce6e8fac863864e4476b54d2050b1625744218\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154867 sha256=7090dc43a0e29344134db88d28d2a532766d8eea2bcc17955477fb1819ad0ad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting stopwords_guilannlp\n",
            "  Downloading https://files.pythonhosted.org/packages/44/bc/a01c003b59a91187e89d11e73e8bb2834bb9ae6b36fe576a4b617c90bd23/stopwords_guilannlp-13.2019.3.5-py3-none-any.whl\n",
            "Installing collected packages: stopwords-guilannlp\n",
            "Successfully installed stopwords-guilannlp-13.2019.3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUO9K1mKO2EB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88541d3-325b-4251-96bd-9177e40ebdcd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJiCXKiUNsUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2810380-82b4-4158-b004-6f3b378b9066"
      },
      "source": [
        "sentence = \"Hello, world!\"\n",
        "print (nltk.word_tokenize(sentence))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aE9xQhhQ6XS"
      },
      "source": [
        "It's the first phase of your final project for the *NLP981* course. The main idea behind this phase is to portray the develope side of *NLP*.\n",
        "\n",
        "You must code inside of this python notebook. I highly recommend you to use the *Google Colab* environment. \n",
        "\n",
        "If you have any questions, feel free to ask.\n",
        "You can use [*Quera*](https://quera.ir/course/4385/) platform for your general questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgS3aCY358dV"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FG4cndz6DDq"
      },
      "source": [
        "A category predictor is going to build at this phase of the project.\n",
        "\n",
        "The predictor gets a text as input and predicts a category for that.\n",
        "\n",
        "For this purpose, you need to :\n",
        "\n",
        "1.   Load the dataset\n",
        "2.   Preprocess the text data\n",
        "3.   Implement a word representation method to represent each text as a numeric vector\n",
        "4.   Implement a classification model and train that using the training set\n",
        "5.   Predict a category for each of validation data using implemented model\n",
        "6.   Measure your work using confusion matrix and some common metrics\n",
        "\n",
        "**Important Note:** You can use any library you want in sections 1 and 2. But everything in section 3-6 need to be coded purely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0acil66KQ8A_"
      },
      "source": [
        "## 1) Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haH1kfj3Q9tf"
      },
      "source": [
        "The dataset you will use in this phase is called *Divar* that released by the *CafeBazaar* research team.\n",
        "\n",
        "It contains more than 900,000 posts of the *Divar* ads platform. We split this dataset into training, validation, and testing sets.\n",
        "\n",
        "The testing set is not accessible for you, and we use them to evaluate your work on the presentation day.\n",
        "\n",
        "You can download the dataset files (training and validation sets) directly from the following link :\n",
        "\n",
        "> *https://drive.google.com/open?id=1oj-fqpymjDr8QsOK-zQliiqXbVqakrFo*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdIRg1UBSOEi"
      },
      "source": [
        "### 1.1) Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwCPJjaSQukm"
      },
      "source": [
        "import pandas as pd\n",
        "#from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "from stopwords_guilannlp import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "import random\n",
        "    \n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "#train_set = pd.read_csv('trainset.csv')\n",
        "#valid_set = pd.read_csv('validationset.csv')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1-D-AmUvc2fFOs53HWxoSRpy-6MtTlHRc'\n",
        "st, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('trainset.csv')  \n",
        "train_set = pd.read_csv('trainset.csv')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1-J4-W7HPdDowThY24hUoqCQGDW7iUNcN'\n",
        "st, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('validationset.csv')  \n",
        "valid_set = pd.read_csv('validationset.csv')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWHwVPkfS-OX"
      },
      "source": [
        "### 1.2) Analyzing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlIKCffqU4AM"
      },
      "source": [
        "Display the top 10 rows of the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0EuzrFVIat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae34b99-a76d-4a7e-f3f3-b024a3e63d61"
      },
      "source": [
        "print(train_set.head(10))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  Unnamed: 0.1  archive_by_user                   brand                cat1                       cat2            cat3     city      created_at                                               desc              id  image_count   mileage platform     price                                     title   type  year\n",
            "0           0        282086             True                     NaN            personal         clothing-and-shoes  shoes-belt-bag   Tehran     Monday 11AM  چکمه یکبار پوشیده شده قیمت 42\\nکفش قهوه ای سوخ...  12875614029625            3       NaN   mobile     42000                                   سایز 40  women   NaN\n",
            "1           1        762753             True        Samsung::سامسونگ  electronic-devices              mobile-tablet   mobile-phones   Tehran  Wednesday 12PM  گوشی رو تا حالا باز نکردم و تو جعبه پلمپه از د...  16051997226596            0       NaN   mobile    850000                      گوشی سامسونگ a3 2016    NaN   NaN\n",
            "2           2        805240             True                     NaN            personal        jewelry-and-watches         watches   Tehran    Tuesday 09AM  ساعت هیچ مشکلی ندارد اصل اصل هستش چون دیگه دست...  51715717387979            2       NaN   mobile    130000                     ساعت زنانه اسپریت اصل    NaN   NaN\n",
            "3           3        556730            False                     NaN            personal              baby-and-toys   personal-toys   Tehran   Thursday 06PM  دوچرخه از هرلحاظ سالمه و فقط مدت کوتاهی استفاد...   9381204619687            2       NaN   mobile    200000  فروش یک عدد دوچرخه مارک پرادو بسیار سالم    NaN   NaN\n",
            "4           4        727332             True                     NaN     leisure-hobbies         hobby-collectibles      coin-stamp   Tehran  Wednesday 09PM  14 اسکناس مطابق تصویر همه باهم 200 هزار تومان\\...  47687757949429            3       NaN      web        -1                  14 اسکناس کلکسیونی code1    NaN   NaN\n",
            "5           5        805039             True                     NaN     leisure-hobbies              sport-leisure     ball-sports   Tehran    Tuesday 11AM  فوتبال دستی حرفه ای تاشو درحد نو فقط 2ماه استف...  35888748102799            3       NaN   mobile    480000                       فوتبال دستی حرفه ای    NaN   NaN\n",
            "6           6        812617            False                     NaN        for-the-home  furniture-and-home-decore        lighting   Tehran   Thursday 08AM                            لوستر نو برای اتاق کودک  61553946516880            1       NaN   mobile     25000                                     لوستر    NaN   NaN\n",
            "7           7        295730             True                     NaN            vehicles          parts-accessories             NaN  Isfahan     Monday 10PM  4 عدد رینگ اهنی ال 90 سالم قیمت از شما من فروش...  21090520055338            0       NaN   mobile        -1                           رینگ اهنی ال 90    NaN   NaN\n",
            "8           8        777605            False           نیسان::Nissan            vehicles                       cars           light   Shiraz  Wednesday 01PM                   بیمه تا۹۵/۹/۲۷,کف دستی رنگ درعقب  27046580911179            2  380000.0   mobile  40000000                        نیسان سرانزا مدل۸۳    NaN  1383\n",
            "9           9        797079            False  پراید صندوق‌دار::Pride            vehicles                       cars           light   Tehran    Tuesday 03PM  عقب وجلوپلمب بیرنگ لاستیک نو به شرط مصرف کننده...  42106059496341            0  109000.0   mobile  14000000                                پراید سفید    NaN  1390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqfkJST6VMXt"
      },
      "source": [
        "How many data (rows) stored in the training and validation sets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkiaz92zVVpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7094cd87-907c-4ee4-b61e-aca2198ce99e"
      },
      "source": [
        "train_len = train_set.shape[0]\n",
        "valid_len = valid_set.shape[0]\n",
        "print('Train set has {} rows.'.format(train_len))\n",
        "print('Validation set has {} rows.'.format(valid_len))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set has 700000 rows.\n",
            "Validation set has 147635 rows.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aw0x_s3_b4U"
      },
      "source": [
        "How many posts are in each category (First level categories)? (Based on training set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31PFgy46_ntw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7767b7-e270-496f-8dc1-d915010916d1"
      },
      "source": [
        "cat1 = train_set[\"cat1\"]\n",
        "#cat2 = train_set[\"cat2\"]\n",
        "#cat3 = train_set[\"cat3\"]\n",
        "\n",
        "#Test set = Valid set\n",
        "test_cat1 = valid_set[\"cat1\"]\n",
        "#test_cat2 = valid_set[\"cat2\"]\n",
        "#test_cat3 = valid_set[\"cat3\"]\n",
        "\n",
        "unique_cat1 = np.unique(cat1)\n",
        "\n",
        "#in the following lines of code we print how many posts are in each category\n",
        "cat1_posts_count = dict.fromkeys(unique_cat1, 0)\n",
        "for cat in cat1:\n",
        "  cat1_posts_count[cat] += 1\n",
        "\n",
        "for cat, cnt in cat1_posts_count.items():\n",
        "  print('Category \"{}\" contains {} posts.'.format(cat, cnt))\n",
        "\n",
        "dic_cat1 = {}\n",
        "for i in range(len(unique_cat1)):\n",
        "    dic_cat1[unique_cat1[i]] = i\n",
        "\n",
        "y_train_cat1 = []\n",
        "y_test_cat1 = []\n",
        "for i in range(len(cat1)):\n",
        "    y_train_cat1.append(dic_cat1[cat1[i]])\n",
        "    \n",
        "for i in range(len(test_cat1)):\n",
        "    y_test_cat1.append(dic_cat1[test_cat1[i]])\n",
        "\n",
        "y_train_cat1 = np.array(list(y_train_cat1))\n",
        "y_test_cat1 = np.array(list(y_test_cat1))\n",
        "\n",
        "#print(y_test_cat1)\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Category \"businesses\" contains 45660 posts.\n",
            "Category \"electronic-devices\" contains 122905 posts.\n",
            "Category \"for-the-home\" contains 214955 posts.\n",
            "Category \"leisure-hobbies\" contains 61676 posts.\n",
            "Category \"personal\" contains 102804 posts.\n",
            "Category \"vehicles\" contains 152000 posts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUejNXljlZD1"
      },
      "source": [
        "## 2) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j1EilpbAi-W"
      },
      "source": [
        "There are two kinds of text data in the dataset: *Title* and *Description*.\n",
        "You can use one or both of them as text inputs of your classification model. Choose a composition that gives you a higher measuring score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLT50bemjw3K"
      },
      "source": [
        "You need to apply some preprocessing procedures on your text data first. We want at least **4** preprocessing step from you. It can be removing stop words, removing punctation, removing or replacing digits, stemming, lemmatizing, normalization, and so on.\n",
        "\n",
        "You can use the [*Stopwords Guilan NLP*](https://github.com/JoyeBright/stopwords_guilannlp) library to access a collection of Persian stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPQYAeEnlrcc"
      },
      "source": [
        "#for faster result, we define the following lines of code outside the 'preprocessing' function\n",
        "normalizer = Normalizer()\n",
        "stopwords = stopwords_output(\"Persian\", \"nar\")\n",
        "lemmatizer = Lemmatizer()\n",
        "stemmer = Stemmer()\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    \n",
        "    text = re.sub('<[^<]+?>','', text)\n",
        "    text = ''.join(c for c in text if not c.isdigit())\n",
        "    text = ''.join(c for c in text if c not in punctuation)\n",
        "    text = normalizer.normalize(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    cleared_text = []\n",
        "    for word in tokens:\n",
        "          word = normalizer.normalize(word)\n",
        "          word = stemmer.stem(word)\n",
        "          word = lemmatizer.lemmatize(word)\n",
        "          #word = re.sub(r'\\d+', '', word)\n",
        "          if word not in stopwords and len(word) > 0:\n",
        "                cleared_text.append(word)\n",
        "    return cleared_text\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izN13hPulonI"
      },
      "source": [
        "## 3) Word Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeC-rd4DMSUb"
      },
      "source": [
        "As you know, classification models can't deal with strings directly, and you have to represent your texts in a numerical form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1rGMFJmqKwm"
      },
      "source": [
        "### 3.1) Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfPOmAF6qOF0"
      },
      "source": [
        "You have to implement the tf-idf vectorization method from scratch in this step. \n",
        "\n",
        "Furthermore, a function must be implemented that gives a text input and return a tf-idf vectorized representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTZCMdqft4Ic"
      },
      "source": [
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* (term-frequency) is the count of occurrences of the word `t` in specific text `d`.\n",
        "\n",
        "*idf* (inverse document-frequency) is term that is inversely proportional to the number of texts with the given word. It can be calculated this way:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "where $n_d$ is the whole number of texts and $n_{d(t)}$ is the number of texts with the word `t`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVwNZTGLqm20"
      },
      "source": [
        "column1 = \"title\"\n",
        "column2 = \"desc\"\n",
        "rows_of_train = 7350\n",
        "rows_of_test = 1750\n",
        "docs1 = train_set.loc[0:rows_of_train-1,column1]\n",
        "docs2 = train_set.loc[0:rows_of_train-1,column2]\n",
        "test_docs1  = valid_set.loc[0:rows_of_test-1,column1]\n",
        "test_docs2 = valid_set.loc[0:rows_of_test-1,column2]\n",
        "logistic_classifiers = []\n",
        "num_classes = len(unique_cat1)\n",
        "losses = []\n",
        "#BoW\n",
        "bow1 = []\n",
        "bow2 = []\n",
        "bow_test1 = []\n",
        "bow_test2 = []\n",
        "for i in range(len(docs1)):\n",
        "    bow1.append(preprocessing(docs1[i]))\n",
        "    bow2.append(preprocessing(docs2[i]))\n",
        "    #print('Documents train {} preprocessed.'.format(i+1))\n",
        "    \n",
        "for i in range(len(test_docs1)):\n",
        "    bow_test1.append(preprocessing(test_docs1[i]))\n",
        "    bow_test2.append(preprocessing(test_docs2[i]))\n",
        "    #print('Documents test {} preprocessed.'.format(i+1))\n",
        "\n",
        "#print(bow)\n",
        "\n",
        "unique_words1 = set(bow1[0]).union(set(bow1[1]))\n",
        "unique_words2 = set(bow2[0]).union(set(bow2[1]))\n",
        "unique_words_test1 = set(bow_test1[0]).union(set(bow_test1[1]))\n",
        "unique_words_test2 = set(bow_test2[0]).union(set(bow_test2[1]))\n",
        "\n",
        "\n",
        "for i in range(2,len(docs1)):\n",
        "    unique_words1 = set(bow1[i]).union(set(unique_words1))\n",
        "    unique_words2 = set(bow2[i]).union(set(unique_words2))\n",
        "\n",
        "for i in range(2,len(test_docs1)):\n",
        "    unique_words_test1 = set(bow_test1[i]).union(set(unique_words_test1))\n",
        "    unique_words_test2 = set(bow_test2[i]).union(set(unique_words_test2))\n",
        "    \n",
        "unique_words1 = set(unique_words1).union(set(unique_words_test1))\n",
        "unique_words2 = set(unique_words2).union(set(unique_words_test2))\n",
        "unique_words = set(unique_words1).union(set(unique_words2))\n",
        "\n",
        "del unique_words1\n",
        "del unique_words2\n",
        "del unique_words_test1\n",
        "del unique_words_test2\n",
        "\n",
        "#print(unique_words)\n",
        "\n",
        "\n",
        "#Now we create a dictionary for each document in TrainSet and TestSet\n",
        "Docs1 = []\n",
        "Docs2 = []\n",
        "Test_Docs1 = []\n",
        "Test_Docs2 = []\n",
        "\n",
        "for i in range(len(docs1)):\n",
        "    Docs1.append(dict.fromkeys(unique_words, 0))\n",
        "    Docs2.append(dict.fromkeys(unique_words, 0))\n",
        "\n",
        "del docs1\n",
        "\n",
        "for i in range(len(test_docs1)):\n",
        "    Test_Docs1.append(dict.fromkeys(unique_words, 0))\n",
        "    Test_Docs2.append(dict.fromkeys(unique_words, 0))\n",
        "\n",
        "del docs2\n",
        "\n",
        "for i in range(len(bow1)):\n",
        "    for word in bow1[i]:\n",
        "        Docs1[i][word] += 1\n",
        "    for word in bow2[i]:\n",
        "        Docs2[i][word] += 1\n",
        "        \n",
        "for i in range(len(bow_test1)):\n",
        "    for word in bow_test1[i]:\n",
        "        Test_Docs1[i][word] += 1\n",
        "    for word in bow_test2[i]:\n",
        "        Test_Docs2[i][word] += 1\n",
        "del bow1\n",
        "del bow2\n",
        "del bow_test1\n",
        "del bow_test2   \n",
        "    \n",
        "#TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
        "def tf(doc):\n",
        "    doc_tf = dict.fromkeys(doc.keys(), 0)\n",
        "    s = sum(doc.values())\n",
        "    if s > 0 :\n",
        "        for word, cnt in doc.items():\n",
        "             doc_tf[word] = float(cnt / s )\n",
        "\n",
        "    return doc_tf\n",
        "\n",
        "\n",
        "\n",
        "#IDF: (Total number of sentences (documents))/(Number of sentences (documents) containing the word)\n",
        "def idf(docs):\n",
        "    #number of documents\n",
        "    nd = len(docs)\n",
        "    docs_idf = dict.fromkeys(docs[0].keys(), 0)\n",
        "    for doc in docs:\n",
        "        for word, cnt in doc.items():\n",
        "            if cnt > 0:\n",
        "                docs_idf[word] += 1\n",
        "                \n",
        "    for word, cnt in docs_idf.items():\n",
        "        docs_idf[word] = math.log(float(float(1+nd)/float(1+cnt))) + 1\n",
        "    return docs_idf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyUQ-_urWW5"
      },
      "source": [
        "#Finally, the TF-IDF values are calculated by multiplying TF values with their corresponding IDF values.\n",
        "def tf_idf_compute(tf, idfs):\n",
        "    tf_idf = {}\n",
        "    for word, value in tf.items():\n",
        "        tf_idf[word] = value * idfs[word]\n",
        "    #returns tf_idf \n",
        "    return tf_idf\n",
        "\n",
        "\n",
        "\n",
        "def tf_idf(text):\n",
        "    tf_idfs = []\n",
        "    text_tfs = []\n",
        "    for doc in text:\n",
        "        text_tfs.append(tf(doc))\n",
        "    text_idfs = idf(text)\n",
        "    for i in range(len(text)):\n",
        "        tf_idfs.append(tf_idf_compute(text_tfs[i], text_idfs))\n",
        "        #print('Document {} tf_idf computed.'.format(i+1))\n",
        "    \n",
        "    #returns tf_idf vectors\n",
        "    return tf_idfs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSDMXv46CWwm"
      },
      "source": [
        "## 4) Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRpciSZZ-mb9"
      },
      "source": [
        "![alt text](https://cdn.lynda.com/course/578082/578082-637075371482276339-16x9.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1EJq7B4JwX"
      },
      "source": [
        "### 4.1) Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoiyVNNz-q5k"
      },
      "source": [
        "The Logistic Regression classifier must be implemented from scratch here.\n",
        "\n",
        "You can fit the training data into the classifier after implementing linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKe-JiutkC3H"
      },
      "source": [
        "#Logistic regression methods and requirements\n",
        "def linear_mult(X, w, b):\n",
        "    return np.dot(w.T, X) + b\n",
        "\n",
        "def init_with_zeros(dimension):\n",
        "    w = np.zeros((dimension,1))\n",
        "    b = 0\n",
        "    return w, b\n",
        "\n",
        "#Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cost_function(y, a):\n",
        "    return -np.mean(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "\n",
        "def forward_propagation(w, b, X, Y):\n",
        "    m = X.shape[1]\n",
        "    Z = linear_mult(X, w, b)\n",
        "    A = sigmoid(Z)                                 \n",
        "    cost = cost_function(Y, A)  \n",
        "    cost = np.squeeze(cost)    \n",
        "    back_require = {\n",
        "        'A': A\n",
        "    }\n",
        "    return back_require, cost\n",
        "\n",
        "def backward_propagation(b, X, Y, back_require):   \n",
        "    m = X.shape[1]\n",
        "    A = back_require['A']\n",
        "    dw = (1/m) * np.dot(X,(A-Y).T)\n",
        "    db = (1/m) * np.sum(A - Y)\n",
        "  \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    #First, forward propagation ;\n",
        "    back_require, cost = forward_propagation(w, b, X, Y)\n",
        "    #then backward propagation\n",
        "    grads = backward_propagation(b, X, Y, back_require)\n",
        "    return grads, cost\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    costs = []\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "        \n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        #Update weights and bias term\n",
        "        w -= learning_rate*dw\n",
        "        b -= learning_rate*db\n",
        "        \n",
        "        # Record the costs values\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost in every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    return params, grads, costs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q-s73zI-xOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8915c528-3391-40e8-d4c3-2514df52fce7"
      },
      "source": [
        "# Training phase\n",
        "del train_set\n",
        "del valid_set\n",
        "\n",
        "X_t1 = []\n",
        "X_t2 = []\n",
        "X_test1 = []\n",
        "X_test2 = []\n",
        "\n",
        "temp1 = tf_idf(Docs1)\n",
        "del Docs1\n",
        "\n",
        "for i in range(len(temp1)):\n",
        "    X_t1.append(np.array(list(temp1[i].values())))\n",
        "del temp1\n",
        "\n",
        "temp2 = tf_idf(Docs2)\n",
        "del Docs2\n",
        "\n",
        "for i in range(len(temp2)):    \n",
        "    X_t2.append(np.array(list(temp2[i].values())))\n",
        "del temp2 \n",
        "\n",
        "Test_Docs1 = tf_idf(Test_Docs1)\n",
        "\n",
        "for i in range(len(Test_Docs1)):\n",
        "    X_test1.append(np.array(list(Test_Docs1[i].values())))\n",
        "del Test_Docs1\n",
        "\n",
        "\n",
        "Test_Docs2 = tf_idf(Test_Docs2)\n",
        "\n",
        "for i in range(len(Test_Docs2)):    \n",
        "    X_test2.append(np.array(list(Test_Docs2[i].values())))    \n",
        "del Test_Docs2\n",
        "\n",
        "\n",
        "X_t1 = np.array(list(X_t1)).T\n",
        "X_t2 = np.array(list(X_t2)).T\n",
        "\n",
        "Y_t = np.array(y_train_cat1[:rows_of_train])\n",
        "del y_train_cat1\n",
        "Y_test = np.array(y_test_cat1[:rows_of_test])\n",
        "del y_test_cat1\n",
        "\n",
        "for i in range(num_classes):\n",
        "    print('Training model for class {} :'.format(i+1))\n",
        "    y_train_logistic = deepcopy(Y_t)\n",
        "    idxs_i = y_train_logistic == i\n",
        "\n",
        "    y_train_logistic[idxs_i] = 1\n",
        "    y_train_logistic[~idxs_i] = 0\n",
        "\n",
        "    w, b = init_with_zeros(len(unique_words))\n",
        "    print('Training on \"{}\" column :'.format(column2))\n",
        "    params, grads, costs = optimize(w, b, X_t2, y_train_logistic, num_iterations = 2500, learning_rate = 0.046, print_cost = True)\n",
        "    #losses.append(costs)\n",
        "    print('Training on \"{}\" column :'.format(column1))\n",
        "    params_new, grads_new, costs_new = optimize(params['w'], params['b'], X_t1, y_train_logistic, num_iterations = 5000, learning_rate = 0.365, print_cost = True)\n",
        "    logistic_classifiers.append(params_new)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model for class 1 :\n",
            "Training on \"desc\" column :\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.325408\n",
            "Cost after iteration 200: 0.264108\n",
            "Cost after iteration 300: 0.245652\n",
            "Cost after iteration 400: 0.238184\n",
            "Cost after iteration 500: 0.234526\n",
            "Cost after iteration 600: 0.232428\n",
            "Cost after iteration 700: 0.231037\n",
            "Cost after iteration 800: 0.229990\n",
            "Cost after iteration 900: 0.229117\n",
            "Cost after iteration 1000: 0.228337\n",
            "Cost after iteration 1100: 0.227609\n",
            "Cost after iteration 1200: 0.226911\n",
            "Cost after iteration 1300: 0.226232\n",
            "Cost after iteration 1400: 0.225564\n",
            "Cost after iteration 1500: 0.224906\n",
            "Cost after iteration 1600: 0.224254\n",
            "Cost after iteration 1700: 0.223608\n",
            "Cost after iteration 1800: 0.222967\n",
            "Cost after iteration 1900: 0.222331\n",
            "Cost after iteration 2000: 0.221700\n",
            "Cost after iteration 2100: 0.221074\n",
            "Cost after iteration 2200: 0.220452\n",
            "Cost after iteration 2300: 0.219835\n",
            "Cost after iteration 2400: 0.219222\n",
            "Training on \"title\" column :\n",
            "Cost after iteration 0: 0.215637\n",
            "Cost after iteration 100: 0.196823\n",
            "Cost after iteration 200: 0.182574\n",
            "Cost after iteration 300: 0.171242\n",
            "Cost after iteration 400: 0.161886\n",
            "Cost after iteration 500: 0.153962\n",
            "Cost after iteration 600: 0.147127\n",
            "Cost after iteration 700: 0.141151\n",
            "Cost after iteration 800: 0.135867\n",
            "Cost after iteration 900: 0.131150\n",
            "Cost after iteration 1000: 0.126905\n",
            "Cost after iteration 1100: 0.123057\n",
            "Cost after iteration 1200: 0.119545\n",
            "Cost after iteration 1300: 0.116324\n",
            "Cost after iteration 1400: 0.113353\n",
            "Cost after iteration 1500: 0.110601\n",
            "Cost after iteration 1600: 0.108042\n",
            "Cost after iteration 1700: 0.105654\n",
            "Cost after iteration 1800: 0.103418\n",
            "Cost after iteration 1900: 0.101319\n",
            "Cost after iteration 2000: 0.099343\n",
            "Cost after iteration 2100: 0.097479\n",
            "Cost after iteration 2200: 0.095716\n",
            "Cost after iteration 2300: 0.094045\n",
            "Cost after iteration 2400: 0.092460\n",
            "Cost after iteration 2500: 0.090952\n",
            "Cost after iteration 2600: 0.089516\n",
            "Cost after iteration 2700: 0.088146\n",
            "Cost after iteration 2800: 0.086838\n",
            "Cost after iteration 2900: 0.085586\n",
            "Cost after iteration 3000: 0.084388\n",
            "Cost after iteration 3100: 0.083239\n",
            "Cost after iteration 3200: 0.082137\n",
            "Cost after iteration 3300: 0.081078\n",
            "Cost after iteration 3400: 0.080059\n",
            "Cost after iteration 3500: 0.079078\n",
            "Cost after iteration 3600: 0.078134\n",
            "Cost after iteration 3700: 0.077223\n",
            "Cost after iteration 3800: 0.076344\n",
            "Cost after iteration 3900: 0.075495\n",
            "Cost after iteration 4000: 0.074675\n",
            "Cost after iteration 4100: 0.073882\n",
            "Cost after iteration 4200: 0.073114\n",
            "Cost after iteration 4300: 0.072370\n",
            "Cost after iteration 4400: 0.071650\n",
            "Cost after iteration 4500: 0.070952\n",
            "Cost after iteration 4600: 0.070274\n",
            "Cost after iteration 4700: 0.069616\n",
            "Cost after iteration 4800: 0.068978\n",
            "Cost after iteration 4900: 0.068357\n",
            "Training model for class 2 :\n",
            "Training on \"desc\" column :\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.486796\n",
            "Cost after iteration 200: 0.456494\n",
            "Cost after iteration 300: 0.447221\n",
            "Cost after iteration 400: 0.441732\n",
            "Cost after iteration 500: 0.437121\n",
            "Cost after iteration 600: 0.432788\n",
            "Cost after iteration 700: 0.428596\n",
            "Cost after iteration 800: 0.424512\n",
            "Cost after iteration 900: 0.420526\n",
            "Cost after iteration 1000: 0.416635\n",
            "Cost after iteration 1100: 0.412837\n",
            "Cost after iteration 1200: 0.409129\n",
            "Cost after iteration 1300: 0.405511\n",
            "Cost after iteration 1400: 0.401979\n",
            "Cost after iteration 1500: 0.398532\n",
            "Cost after iteration 1600: 0.395168\n",
            "Cost after iteration 1700: 0.391885\n",
            "Cost after iteration 1800: 0.388680\n",
            "Cost after iteration 1900: 0.385552\n",
            "Cost after iteration 2000: 0.382498\n",
            "Cost after iteration 2100: 0.379515\n",
            "Cost after iteration 2200: 0.376603\n",
            "Cost after iteration 2300: 0.373759\n",
            "Cost after iteration 2400: 0.370980\n",
            "Training on \"title\" column :\n",
            "Cost after iteration 0: 0.334986\n",
            "Cost after iteration 100: 0.274536\n",
            "Cost after iteration 200: 0.238014\n",
            "Cost after iteration 300: 0.212694\n",
            "Cost after iteration 400: 0.193750\n",
            "Cost after iteration 500: 0.178883\n",
            "Cost after iteration 600: 0.166821\n",
            "Cost after iteration 700: 0.156791\n",
            "Cost after iteration 800: 0.148287\n",
            "Cost after iteration 900: 0.140963\n",
            "Cost after iteration 1000: 0.134573\n",
            "Cost after iteration 1100: 0.128936\n",
            "Cost after iteration 1200: 0.123918\n",
            "Cost after iteration 1300: 0.119413\n",
            "Cost after iteration 1400: 0.115342\n",
            "Cost after iteration 1500: 0.111640\n",
            "Cost after iteration 1600: 0.108254\n",
            "Cost after iteration 1700: 0.105144\n",
            "Cost after iteration 1800: 0.102273\n",
            "Cost after iteration 1900: 0.099614\n",
            "Cost after iteration 2000: 0.097142\n",
            "Cost after iteration 2100: 0.094836\n",
            "Cost after iteration 2200: 0.092680\n",
            "Cost after iteration 2300: 0.090657\n",
            "Cost after iteration 2400: 0.088755\n",
            "Cost after iteration 2500: 0.086962\n",
            "Cost after iteration 2600: 0.085270\n",
            "Cost after iteration 2700: 0.083668\n",
            "Cost after iteration 2800: 0.082150\n",
            "Cost after iteration 2900: 0.080708\n",
            "Cost after iteration 3000: 0.079337\n",
            "Cost after iteration 3100: 0.078031\n",
            "Cost after iteration 3200: 0.076786\n",
            "Cost after iteration 3300: 0.075596\n",
            "Cost after iteration 3400: 0.074459\n",
            "Cost after iteration 3500: 0.073369\n",
            "Cost after iteration 3600: 0.072326\n",
            "Cost after iteration 3700: 0.071324\n",
            "Cost after iteration 3800: 0.070362\n",
            "Cost after iteration 3900: 0.069438\n",
            "Cost after iteration 4000: 0.068548\n",
            "Cost after iteration 4100: 0.067691\n",
            "Cost after iteration 4200: 0.066866\n",
            "Cost after iteration 4300: 0.066069\n",
            "Cost after iteration 4400: 0.065300\n",
            "Cost after iteration 4500: 0.064558\n",
            "Cost after iteration 4600: 0.063840\n",
            "Cost after iteration 4700: 0.063146\n",
            "Cost after iteration 4800: 0.062474\n",
            "Cost after iteration 4900: 0.061823\n",
            "Training model for class 3 :\n",
            "Training on \"desc\" column :\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.613519\n",
            "Cost after iteration 200: 0.597645\n",
            "Cost after iteration 300: 0.588430\n",
            "Cost after iteration 400: 0.580339\n",
            "Cost after iteration 500: 0.572727\n",
            "Cost after iteration 600: 0.565496\n",
            "Cost after iteration 700: 0.558614\n",
            "Cost after iteration 800: 0.552058\n",
            "Cost after iteration 900: 0.545807\n",
            "Cost after iteration 1000: 0.539842\n",
            "Cost after iteration 1100: 0.534142\n",
            "Cost after iteration 1200: 0.528692\n",
            "Cost after iteration 1300: 0.523474\n",
            "Cost after iteration 1400: 0.518473\n",
            "Cost after iteration 1500: 0.513675\n",
            "Cost after iteration 1600: 0.509067\n",
            "Cost after iteration 1700: 0.504637\n",
            "Cost after iteration 1800: 0.500373\n",
            "Cost after iteration 1900: 0.496265\n",
            "Cost after iteration 2000: 0.492305\n",
            "Cost after iteration 2100: 0.488482\n",
            "Cost after iteration 2200: 0.484789\n",
            "Cost after iteration 2300: 0.481219\n",
            "Cost after iteration 2400: 0.477765\n",
            "Training on \"title\" column :\n",
            "Cost after iteration 0: 0.403530\n",
            "Cost after iteration 100: 0.335038\n",
            "Cost after iteration 200: 0.295914\n",
            "Cost after iteration 300: 0.269243\n",
            "Cost after iteration 400: 0.249456\n",
            "Cost after iteration 500: 0.233968\n",
            "Cost after iteration 600: 0.221387\n",
            "Cost after iteration 700: 0.210884\n",
            "Cost after iteration 800: 0.201933\n",
            "Cost after iteration 900: 0.194178\n",
            "Cost after iteration 1000: 0.187368\n",
            "Cost after iteration 1100: 0.181322\n",
            "Cost after iteration 1200: 0.175904\n",
            "Cost after iteration 1300: 0.171010\n",
            "Cost after iteration 1400: 0.166559\n",
            "Cost after iteration 1500: 0.162486\n",
            "Cost after iteration 1600: 0.158741\n",
            "Cost after iteration 1700: 0.155281\n",
            "Cost after iteration 1800: 0.152070\n",
            "Cost after iteration 1900: 0.149080\n",
            "Cost after iteration 2000: 0.146286\n",
            "Cost after iteration 2100: 0.143667\n",
            "Cost after iteration 2200: 0.141207\n",
            "Cost after iteration 2300: 0.138888\n",
            "Cost after iteration 2400: 0.136699\n",
            "Cost after iteration 2500: 0.134627\n",
            "Cost after iteration 2600: 0.132662\n",
            "Cost after iteration 2700: 0.130796\n",
            "Cost after iteration 2800: 0.129020\n",
            "Cost after iteration 2900: 0.127328\n",
            "Cost after iteration 3000: 0.125712\n",
            "Cost after iteration 3100: 0.124168\n",
            "Cost after iteration 3200: 0.122690\n",
            "Cost after iteration 3300: 0.121274\n",
            "Cost after iteration 3400: 0.119916\n",
            "Cost after iteration 3500: 0.118611\n",
            "Cost after iteration 3600: 0.117357\n",
            "Cost after iteration 3700: 0.116150\n",
            "Cost after iteration 3800: 0.114987\n",
            "Cost after iteration 3900: 0.113866\n",
            "Cost after iteration 4000: 0.112785\n",
            "Cost after iteration 4100: 0.111741\n",
            "Cost after iteration 4200: 0.110732\n",
            "Cost after iteration 4300: 0.109756\n",
            "Cost after iteration 4400: 0.108811\n",
            "Cost after iteration 4500: 0.107896\n",
            "Cost after iteration 4600: 0.107010\n",
            "Cost after iteration 4700: 0.106151\n",
            "Cost after iteration 4800: 0.105317\n",
            "Cost after iteration 4900: 0.104507\n",
            "Training model for class 4 :\n",
            "Training on \"desc\" column :\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.379180\n",
            "Cost after iteration 200: 0.329606\n",
            "Cost after iteration 300: 0.315402\n",
            "Cost after iteration 400: 0.309390\n",
            "Cost after iteration 500: 0.305874\n",
            "Cost after iteration 600: 0.303245\n",
            "Cost after iteration 700: 0.300967\n",
            "Cost after iteration 800: 0.298846\n",
            "Cost after iteration 900: 0.296805\n",
            "Cost after iteration 1000: 0.294814\n",
            "Cost after iteration 1100: 0.292860\n",
            "Cost after iteration 1200: 0.290939\n",
            "Cost after iteration 1300: 0.289048\n",
            "Cost after iteration 1400: 0.287187\n",
            "Cost after iteration 1500: 0.285354\n",
            "Cost after iteration 1600: 0.283549\n",
            "Cost after iteration 1700: 0.281773\n",
            "Cost after iteration 1800: 0.280025\n",
            "Cost after iteration 1900: 0.278304\n",
            "Cost after iteration 2000: 0.276612\n",
            "Cost after iteration 2100: 0.274947\n",
            "Cost after iteration 2200: 0.273309\n",
            "Cost after iteration 2300: 0.271698\n",
            "Cost after iteration 2400: 0.270114\n",
            "Training on \"title\" column :\n",
            "Cost after iteration 0: 0.243127\n",
            "Cost after iteration 100: 0.203514\n",
            "Cost after iteration 200: 0.178933\n",
            "Cost after iteration 300: 0.161410\n",
            "Cost after iteration 400: 0.147977\n",
            "Cost after iteration 500: 0.137210\n",
            "Cost after iteration 600: 0.128318\n",
            "Cost after iteration 700: 0.120814\n",
            "Cost after iteration 800: 0.114375\n",
            "Cost after iteration 900: 0.108773\n",
            "Cost after iteration 1000: 0.103843\n",
            "Cost after iteration 1100: 0.099461\n",
            "Cost after iteration 1200: 0.095533\n",
            "Cost after iteration 1300: 0.091985\n",
            "Cost after iteration 1400: 0.088759\n",
            "Cost after iteration 1500: 0.085809\n",
            "Cost after iteration 1600: 0.083097\n",
            "Cost after iteration 1700: 0.080593\n",
            "Cost after iteration 1800: 0.078272\n",
            "Cost after iteration 1900: 0.076112\n",
            "Cost after iteration 2000: 0.074095\n",
            "Cost after iteration 2100: 0.072206\n",
            "Cost after iteration 2200: 0.070433\n",
            "Cost after iteration 2300: 0.068764\n",
            "Cost after iteration 2400: 0.067189\n",
            "Cost after iteration 2500: 0.065700\n",
            "Cost after iteration 2600: 0.064290\n",
            "Cost after iteration 2700: 0.062952\n",
            "Cost after iteration 2800: 0.061681\n",
            "Cost after iteration 2900: 0.060471\n",
            "Cost after iteration 3000: 0.059317\n",
            "Cost after iteration 3100: 0.058216\n",
            "Cost after iteration 3200: 0.057164\n",
            "Cost after iteration 3300: 0.056157\n",
            "Cost after iteration 3400: 0.055193\n",
            "Cost after iteration 3500: 0.054269\n",
            "Cost after iteration 3600: 0.053381\n",
            "Cost after iteration 3700: 0.052529\n",
            "Cost after iteration 3800: 0.051710\n",
            "Cost after iteration 3900: 0.050921\n",
            "Cost after iteration 4000: 0.050162\n",
            "Cost after iteration 4100: 0.049430\n",
            "Cost after iteration 4200: 0.048724\n",
            "Cost after iteration 4300: 0.048043\n",
            "Cost after iteration 4400: 0.047385\n",
            "Cost after iteration 4500: 0.046749\n",
            "Cost after iteration 4600: 0.046135\n",
            "Cost after iteration 4700: 0.045540\n",
            "Cost after iteration 4800: 0.044964\n",
            "Cost after iteration 4900: 0.044406\n",
            "Training model for class 5 :\n",
            "Training on \"desc\" column :\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.449680\n",
            "Cost after iteration 200: 0.412370\n",
            "Cost after iteration 300: 0.400805\n",
            "Cost after iteration 400: 0.394385\n",
            "Cost after iteration 500: 0.389338\n",
            "Cost after iteration 600: 0.384785\n",
            "Cost after iteration 700: 0.380496\n",
            "Cost after iteration 800: 0.376401\n",
            "Cost after iteration 900: 0.372476\n",
            "Cost after iteration 1000: 0.368708\n",
            "Cost after iteration 1100: 0.365089\n",
            "Cost after iteration 1200: 0.361611\n",
            "Cost after iteration 1300: 0.358266\n",
            "Cost after iteration 1400: 0.355047\n",
            "Cost after iteration 1500: 0.351946\n",
            "Cost after iteration 1600: 0.348959\n",
            "Cost after iteration 1700: 0.346077\n",
            "Cost after iteration 1800: 0.343297\n",
            "Cost after iteration 1900: 0.340611\n",
            "Cost after iteration 2000: 0.338015\n",
            "Cost after iteration 2100: 0.335505\n",
            "Cost after iteration 2200: 0.333075\n",
            "Cost after iteration 2300: 0.330722\n",
            "Cost after iteration 2400: 0.328441\n",
            "Training on \"title\" column :\n",
            "Cost after iteration 0: 0.282922\n",
            "Cost after iteration 100: 0.232637\n",
            "Cost after iteration 200: 0.203724\n",
            "Cost after iteration 300: 0.184195\n",
            "Cost after iteration 400: 0.169809\n",
            "Cost after iteration 500: 0.158603\n",
            "Cost after iteration 600: 0.149533\n",
            "Cost after iteration 700: 0.141986\n",
            "Cost after iteration 800: 0.135571\n",
            "Cost after iteration 900: 0.130026\n",
            "Cost after iteration 1000: 0.125167\n",
            "Cost after iteration 1100: 0.120861\n",
            "Cost after iteration 1200: 0.117007\n",
            "Cost after iteration 1300: 0.113529\n",
            "Cost after iteration 1400: 0.110368\n",
            "Cost after iteration 1500: 0.107477\n",
            "Cost after iteration 1600: 0.104819\n",
            "Cost after iteration 1700: 0.102362\n",
            "Cost after iteration 1800: 0.100082\n",
            "Cost after iteration 1900: 0.097959\n",
            "Cost after iteration 2000: 0.095974\n",
            "Cost after iteration 2100: 0.094113\n",
            "Cost after iteration 2200: 0.092364\n",
            "Cost after iteration 2300: 0.090714\n",
            "Cost after iteration 2400: 0.089156\n",
            "Cost after iteration 2500: 0.087682\n",
            "Cost after iteration 2600: 0.086282\n",
            "Cost after iteration 2700: 0.084953\n",
            "Cost after iteration 2800: 0.083688\n",
            "Cost after iteration 2900: 0.082482\n",
            "Cost after iteration 3000: 0.081330\n",
            "Cost after iteration 3100: 0.080230\n",
            "Cost after iteration 3200: 0.079176\n",
            "Cost after iteration 3300: 0.078167\n",
            "Cost after iteration 3400: 0.077199\n",
            "Cost after iteration 3500: 0.076269\n",
            "Cost after iteration 3600: 0.075375\n",
            "Cost after iteration 3700: 0.074515\n",
            "Cost after iteration 3800: 0.073687\n",
            "Cost after iteration 3900: 0.072889\n",
            "Cost after iteration 4000: 0.072119\n",
            "Cost after iteration 4100: 0.071376\n",
            "Cost after iteration 4200: 0.070658\n",
            "Cost after iteration 4300: 0.069964\n",
            "Cost after iteration 4400: 0.069293\n",
            "Cost after iteration 4500: 0.068643\n",
            "Cost after iteration 4600: 0.068014\n",
            "Cost after iteration 4700: 0.067404\n",
            "Cost after iteration 4800: 0.066812\n",
            "Cost after iteration 4900: 0.066238\n",
            "Training model for class 6 :\n",
            "Training on \"desc\" column :\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.535569\n",
            "Cost after iteration 200: 0.508220\n",
            "Cost after iteration 300: 0.495693\n",
            "Cost after iteration 400: 0.485776\n",
            "Cost after iteration 500: 0.476686\n",
            "Cost after iteration 600: 0.468111\n",
            "Cost after iteration 700: 0.459978\n",
            "Cost after iteration 800: 0.452256\n",
            "Cost after iteration 900: 0.444923\n",
            "Cost after iteration 1000: 0.437955\n",
            "Cost after iteration 1100: 0.431332\n",
            "Cost after iteration 1200: 0.425030\n",
            "Cost after iteration 1300: 0.419031\n",
            "Cost after iteration 1400: 0.413315\n",
            "Cost after iteration 1500: 0.407863\n",
            "Cost after iteration 1600: 0.402658\n",
            "Cost after iteration 1700: 0.397684\n",
            "Cost after iteration 1800: 0.392926\n",
            "Cost after iteration 1900: 0.388370\n",
            "Cost after iteration 2000: 0.384003\n",
            "Cost after iteration 2100: 0.379813\n",
            "Cost after iteration 2200: 0.375790\n",
            "Cost after iteration 2300: 0.371922\n",
            "Cost after iteration 2400: 0.368200\n",
            "Training on \"title\" column :\n",
            "Cost after iteration 0: 0.364787\n",
            "Cost after iteration 100: 0.284442\n",
            "Cost after iteration 200: 0.244534\n",
            "Cost after iteration 300: 0.217993\n",
            "Cost after iteration 400: 0.198368\n",
            "Cost after iteration 500: 0.183009\n",
            "Cost after iteration 600: 0.170548\n",
            "Cost after iteration 700: 0.160176\n",
            "Cost after iteration 800: 0.151374\n",
            "Cost after iteration 900: 0.143789\n",
            "Cost after iteration 1000: 0.137169\n",
            "Cost after iteration 1100: 0.131328\n",
            "Cost after iteration 1200: 0.126127\n",
            "Cost after iteration 1300: 0.121459\n",
            "Cost after iteration 1400: 0.117240\n",
            "Cost after iteration 1500: 0.113404\n",
            "Cost after iteration 1600: 0.109897\n",
            "Cost after iteration 1700: 0.106674\n",
            "Cost after iteration 1800: 0.103701\n",
            "Cost after iteration 1900: 0.100947\n",
            "Cost after iteration 2000: 0.098386\n",
            "Cost after iteration 2100: 0.095997\n",
            "Cost after iteration 2200: 0.093763\n",
            "Cost after iteration 2300: 0.091668\n",
            "Cost after iteration 2400: 0.089697\n",
            "Cost after iteration 2500: 0.087840\n",
            "Cost after iteration 2600: 0.086086\n",
            "Cost after iteration 2700: 0.084425\n",
            "Cost after iteration 2800: 0.082851\n",
            "Cost after iteration 2900: 0.081356\n",
            "Cost after iteration 3000: 0.079934\n",
            "Cost after iteration 3100: 0.078579\n",
            "Cost after iteration 3200: 0.077286\n",
            "Cost after iteration 3300: 0.076050\n",
            "Cost after iteration 3400: 0.074869\n",
            "Cost after iteration 3500: 0.073737\n",
            "Cost after iteration 3600: 0.072652\n",
            "Cost after iteration 3700: 0.071610\n",
            "Cost after iteration 3800: 0.070609\n",
            "Cost after iteration 3900: 0.069647\n",
            "Cost after iteration 4000: 0.068720\n",
            "Cost after iteration 4100: 0.067828\n",
            "Cost after iteration 4200: 0.066967\n",
            "Cost after iteration 4300: 0.066136\n",
            "Cost after iteration 4400: 0.065334\n",
            "Cost after iteration 4500: 0.064559\n",
            "Cost after iteration 4600: 0.063809\n",
            "Cost after iteration 4700: 0.063084\n",
            "Cost after iteration 4800: 0.062381\n",
            "Cost after iteration 4900: 0.061700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k089_muZ6j-j"
      },
      "source": [
        "## 5) Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc5dl49O3aST"
      },
      "source": [
        "Now you can predict a category for each of the validation data using the implemented classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bM9Shkx3x7Z"
      },
      "source": [
        "def predict(w, b, X):\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    Z = linear_mult(X, w, b)\n",
        "    A = sigmoid(Z)\n",
        "    for i in range(m):\n",
        "       # Y_prediction[0][i] = 1 if A[0][i] > .5 else 0    \n",
        "        Y_prediction[0][i] = A[0][i]\n",
        "    return Y_prediction\n",
        "\n",
        "def predict_one_vs_all(logistic_classifiers, X, num_classes):\n",
        "    scores = np.zeros((num_classes, X.shape[1]))\n",
        "    for i in range(num_classes):\n",
        "        logistic = logistic_classifiers[i]\n",
        "        scores[i, : ] = predict(logistic['w'], logistic['b'], X)\n",
        "    pred_X = np.argmax(scores, axis=0)\n",
        "    return pred_X\n",
        "\n",
        "\n",
        "X_test1 = np.array(list(X_test1)).T\n",
        "X_test2 = np.array(list(X_test2)).T\n",
        "\n",
        "pred_train1_one_vs_all = predict_one_vs_all(logistic_classifiers, X_t1, num_classes)\n",
        "pred_train2_one_vs_all = predict_one_vs_all(logistic_classifiers, X_t2, num_classes)\n",
        "\n",
        "pred_test1_one_vs_all = predict_one_vs_all(logistic_classifiers, X_test1, num_classes)\n",
        "pred_test2_one_vs_all = predict_one_vs_all(logistic_classifiers, X_test2, num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsxqAwVvwOdD"
      },
      "source": [
        "## 6) Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Ar7r9pyRLq"
      },
      "source": [
        "It's time to evaluate your model using predicted categories for validation data.\n",
        "\n",
        "You need to create a confusion matrix based on your prediction and the real labels. Then you can use this confusion matrix for calculation other measuring metrics. \n",
        "\n",
        "As this problem is a multi-class problem, the calculation formula is a little different from the general case. Read [this article](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-19cA3wiqT"
      },
      "source": [
        "### 6.1) Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB5CCVKbw2Po",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "69ddcbad-8d8d-4ac4-e0a4-f029df9cb526"
      },
      "source": [
        "mpl.style.use('seaborn')\n",
        "\n",
        "conf_arr = np.zeros((num_classes, num_classes))\n",
        "\n",
        "for i in range(len(pred_test1_one_vs_all)):\n",
        "        conf_arr[pred_test1_one_vs_all[i]][Y_test[i]] += 1\n",
        "\n",
        "print(conf_arr)\n",
        "\n",
        "summ = conf_arr.sum()\n",
        "\n",
        "#conf_arr = conf_arr * 100.0 / ( 1.0 * summ )\n",
        "\n",
        "df_cm = pd.DataFrame(conf_arr, \n",
        "  index = unique_cat1,\n",
        "  columns = unique_cat1)\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_aspect(1)\n",
        "\n",
        "cmap = sb.cubehelix_palette(light=1, as_cmap=True)\n",
        "\n",
        "res = sb.heatmap(df_cm, annot=True, vmin=0.0, vmax=np.max(conf_arr), fmt='.2f', cmap=cmap)\n",
        "\n",
        "res.invert_yaxis()\n",
        "\n",
        "plt.yticks([0.5,1.5,2.5,3.5,4.5,5.5], unique_cat1,va='center')\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "plt.savefig('confusion_matrix.png', dpi=700, bbox_inches='tight' )\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 55.   1.   6.   4.   3.   3.]\n",
            " [  3. 245.   3.   2.   3.   5.]\n",
            " [ 37.  42. 512.  27.  30.  44.]\n",
            " [  2.   0.   2. 112.   4.   0.]\n",
            " [  6.   3.  14.   6. 221.   3.]\n",
            " [  5.   7.  13.   5.   4. 318.]]\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM6GgtHNwl1a"
      },
      "source": [
        "### 6.2) Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hL4wy510lEw"
      },
      "source": [
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXPscPgkw4_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0077113-5fde-4dfb-d78a-79920f1e58b3"
      },
      "source": [
        "# Compute the accuracy of the training data and validation data\n",
        "\n",
        "corrects = 0\n",
        "for i in range(len(pred_train1_one_vs_all)):\n",
        "    if int(Y_t[i]) is int(pred_train1_one_vs_all[i]):\n",
        "        corrects += 1\n",
        "        \n",
        "accuracy = float(corrects / len(pred_train1_one_vs_all))*100\n",
        "print('Training dataset accuracy (using \"{}\" column): {} %'.format (column1, accuracy))\n",
        "\n",
        "corrects = 0\n",
        "for i in range(len(pred_test1_one_vs_all)):\n",
        "    if int(Y_test[i]) is int(pred_test1_one_vs_all[i]):\n",
        "        corrects += 1\n",
        "        \n",
        "accuracy = float(corrects / len(Y_test))*100\n",
        "\n",
        "print ('Validation dataset accuracy (using \"{}\" column): {} % '.format (column1, accuracy))\n",
        "\n",
        "#print 'Test datast accuracy: %f' % (np.mean(y_test == pred_test_one_vs_all))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset accuracy (using \"title\" column): 96.0408163265306 %\n",
            "Validation dataset accuracy (using \"title\" column): 83.6 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTFur0U5wsmO"
      },
      "source": [
        "### 6.3) Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frrTkQWG09fd"
      },
      "source": [
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1SGaY9qw7Z4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed18bf2-7fc1-4eb2-c6f2-6e9e2ac3ec71"
      },
      "source": [
        "#Similar to the binary case, we can define precision for each of the classes\n",
        "precision = []\n",
        "#we append the precision for each class to the variable 'precision'\n",
        "for i in range(num_classes):\n",
        "  precision.append(conf_arr[i][i] / np.sum(conf_arr[i], axis = 0))\n",
        "\n",
        "print('Precisions per classes are : {}'.format(precision))\n",
        "\n",
        "macro_precision = 100 * np.sum(precision) / num_classes\n",
        "\n",
        "print('Macro averaged Precision is : {} %'.format(macro_precision))\n",
        "\n",
        "weighted_precision = 0\n",
        "total = 0\n",
        "for i in range(num_classes):\n",
        "  weighted_precision += np.sum(conf_arr, axis = 0)[i] * precision[i]\n",
        "  total += np.sum(conf_arr, axis = 0)[i]\n",
        "\n",
        "weighted_precision = 100 * weighted_precision / total\n",
        "print('Weighted Precision is : {} %'.format(weighted_precision))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precisions per classes are : [0.7638888888888888, 0.9386973180076629, 0.7398843930635838, 0.9333333333333333, 0.8735177865612648, 0.9034090909090909]\n",
            "Macro averaged Precision is : 85.87884684606372 %\n",
            "Weighted Precision is : 84.75554407423319 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQcpDrm1wuhU"
      },
      "source": [
        "### 6.4) Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBb1eop61JlG"
      },
      "source": [
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8bLKc6Ew_iR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a62bc79-4a7d-4ca5-d659-07dfd0643ada"
      },
      "source": [
        "#Similar to the binary case, we can define recall for each of the classes\n",
        "recall = []\n",
        "#We append the recall for each class to the variable 'recall'\n",
        "for i in range(num_classes):\n",
        "  recall.append(conf_arr[i][i] / np.sum(conf_arr, axis = 0)[i])\n",
        "  \n",
        "print('Recalls per classes are : {}'.format(recall))\n",
        "\n",
        "macro_recall = 100 * np.sum(recall) / num_classes\n",
        "print('Macro averaged Recall is : {} %'.format(macro_recall))\n",
        "\n",
        "weighted_recall = 0\n",
        "for i in range(num_classes):\n",
        "  weighted_recall += np.sum(conf_arr, axis = 0)[i] * recall[i]\n",
        "\n",
        "weighted_recall = 100 * weighted_recall / total\n",
        "print('Weighted Recall is : {} %'.format(weighted_recall))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recalls per classes are : [0.5092592592592593, 0.8221476510067114, 0.9309090909090909, 0.717948717948718, 0.8339622641509434, 0.8525469168900804]\n",
            "Macro averaged Recall is : 77.77956500274672 %\n",
            "Weighted Recall is : 83.6 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWx42Dr5wx4m"
      },
      "source": [
        "### 6.5) F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdVnZfm21SCM"
      },
      "source": [
        "$$\\text{F1 score} = 2\\times \\frac{(Recall \\times  Precision)}{Recall + Precision}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkCog8ExBXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33c0355-422e-4ee1-f355-9d9cd1169fca"
      },
      "source": [
        "#F1-score is a function of precision and recall\n",
        "#we can now compute the per-class F1-score\n",
        "f1_score = []\n",
        "for i in range(num_classes):\n",
        "  f1_score.append( 2*( (recall[i]*precision[i]) / (recall[i]+precision[i])))\n",
        "\n",
        "print('F1-scores per classes are : {}'.format(f1_score))\n",
        "\n",
        "macro_f1 = 100 * np.sum(f1_score) / num_classes\n",
        "\n",
        "print('Macro averaged F1-score is : {} %'.format(macro_f1))\n",
        "\n",
        "weighted_f1 = 0\n",
        "for i in range(num_classes):\n",
        "  weighted_f1 += np.sum(conf_arr, axis = 0)[i] * f1_score[i]\n",
        "\n",
        "weighted_f1 = 100 * weighted_f1 / total\n",
        "print('Weighted F1-score is : {} %'.format(weighted_f1))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-scores per classes are : [0.6111111111111112, 0.8765652951699464, 0.8244766505636071, 0.8115942028985509, 0.8532818532818534, 0.8772413793103448]\n",
            "Macro averaged F1-score is : 80.90450820559023 %\n",
            "Weighted F1-score is : 83.46388783001437 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZP1GHbtB2T7"
      },
      "source": [
        "## 7) K-Fold Cross Validation *(Optional)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayL96oOoB-aq"
      },
      "source": [
        "Evaluate your model based on the K-Fold Cross Validation approach. This step is optional and has a few extra points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zxs_90TC-3b"
      },
      "source": [
        "# Your implementation"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}